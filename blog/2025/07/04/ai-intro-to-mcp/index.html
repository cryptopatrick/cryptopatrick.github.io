<!DOCTYPE html>
<html lang="en-EN">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=60544&amp;path=livereload" data-no-instant defer></script>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta content="keyword 1, keyword 2, keyword 3" name="keywords">
<meta content="CryptoPatrick" name="author">
<meta property="og:title" content="ðŸ¤– Ai: Intro to MCP - Seamless Global Nature">
<meta property="og:url" content="http://localhost:60544/blog/2025/07/04/ai-intro-to-mcp/">
<meta property="og:description" content="">
<meta property="og:type" content="website" />


<meta property="og:image" content="http://localhost:60544/img/post-name/name.jpg" />


<title>ðŸ¤– Ai: Intro to MCP | Seamless Global Nature</title>

<link rel="stylesheet" href="http://localhost:60544//css/style.css">
<link rel="stylesheet" href="http://localhost:60544//css/customcode.css">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css">

</head>

<body>
<section class="section">
  <div class="container">
    <nav class="nav">

 <img src="../../../../../profile.jpg" alt="Avatar" style="margin-right: 1em" height="100px">
      <div class="nav-left" style="flex-basis: auto;">
        <a class="nav-item" href="http://localhost:60544/"><h1 class="title is-4">Seamless Global Nature</h1></a>
      <nav class="nav-item level is-mobile">
          
          
          <a class="level-item" href="http://localhost:60544/about/" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-male"></i>
            </span>
            
            About Me
          </a>
          
          <a class="level-item" href="http://localhost:60544/tags/" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-tags"></i>
            </span>
            
            Tags
          </a>
          
          <a class="level-item" href="https://100-days-of-vibe.vercel.app" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-paper-plane"></i>
            </span>
            
            100DaysOfVibe
          </a>
          
          <a class="level-item" href="https://x.com/cryptopatrick" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-twitter"></i>
            </span>
            
             
          </a>
          
          <a class="level-item" href="https://github.com/cryptopatrick?tab=repositories" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-github"></i>
            </span>
            
             
          </a>
          
          <a class="level-item" href="https://www.instagram.com/cryptopatrickk/" style="display: flex; align-items: center;">
            
            <span class="icon" style="margin-right: 0.175em; display: flex; align-items: center;">
              <i class="fa fa-instagram"></i>
            </span>
            
             
          </a>
          
        </nav>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile">
          
        </nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <h1 class="title">ðŸ¤– Ai: Intro to MCP</h1>
    
    
    <h2 class="subtitle is-5">
      July 4, 2025
       by CryptoPatrick
      
        
        <span class="tags-inline" style="margin-left:0.1em;">
          
            
            <a class="tag is-link is-light" href="../../../../../tags/" style="margin-left:0.25em;"></a>
          
            
            <a class="tag is-link is-light" href="../../../../../tags/" style="margin-left:0.25em;"></a>
          
        </span>
      
      
    </h2>
    <div class="content">
      <p><img src="https://img.shields.io/badge/Work_In_Progress-orange.svg" alt="Version:0.1.0">
<img src="https://img.shields.io/badge/Text_Version-0.4.5-blue.svg" alt="Version:0.1.0">
<img src="https://img.shields.io/badge/Source_Code_Builds-Yes-green.svg" alt="Version:0.1.0"></p>
<!--![Version:0.1.0](https://img.shields.io/badge/Version-0.5.0-white.svg)  -->
<!--[![Version:0.1.0](https://img.shields.io/badge/File-issue-orange.svg)](https://github.com/cryptopatrick/learn_wgpu/issues)-->
<p>Model Context Protocol (MCP) was created and released by the company Anthropic
in 2024. It solves a problem - standardising the way LLM-based AI communicates
with the outside (online) world.</p>
<p>Let&rsquo;s set the context: ChatGPT (or any other LLM) can generate a great cover letter for your job application, but it can&rsquo;t email it to the company.
The LLM is &lsquo;sandboxed&rsquo; in the sense that it cannot <strong>interact</strong> with anything outside itself - be it a querying an external database, browser to the URL and fill out a form, editing a calendar, or send an invite to a meeting.</p>
<p>Model Context Protocol (MCP) changes this.</p>
<blockquote>
<p>Yes, it&rsquo;s true that ChatGPT and other LLM platforms like Gemini, can search the web, but that&rsquo;s because the user interface simply adds a web query function like with any browser - it&rsquo;s not the model itself that is querying the web.</p>
</blockquote>
<p>Let&rsquo;s break down MCP into parts.</p>
<h3 id="model">Model</h3>
<p>The actual LLM model.</p>
<h3 id="context">Context</h3>
<p>Outside information via traditional APIs, like OpenWeather API.</p>
<h3 id="protocol">Protocol</h3>
<p>Standardising rules so that everyone can cooperate and communicate using MCP.</p>
<p>Model Context Protocol (MCP) is a standardized way to connect LLMs and Artificial Intelligence Agents to outside sources.</p>
<h3 id="web-apis">Web APIs</h3>
<p>There are thousands of popular web APIs that we can connect to using MCP, for example: Google Maps API, Twitter API, OpenWeatherMap API, Star Wars API (<a href="https://swapi.dev">https://swapi.dev</a>), or PokÃ©mon API (<a href="https://pokeapi.co">https://pokeapi.co</a>).</p>
<h3 id="datasources">Datasources</h3>
<h3 id="local-file-system">Local File System</h3>
<p>The <em>MCP Server</em> wraps the above mentioned sources as <em>MCP Tools</em>, and makes them available to the MCP client.</p>
<h2 id="mcp-client">MCP Client</h2>
<p>An Ai application can use the MCP client to communicate with the
MCP Server. These MCP Clients are wrapped inside <em>MCP Hosts</em>, like for example: <em>Claude Desktop</em> and <em>Cursor IDE</em>.</p>
<p>ðŸ§ª These MCP Hosts typically sends our prompt &ldquo;Which city is currently warmest, Paris, New York, or Beijin?&rdquo;, external LLM (calling an API).</p>
<blockquote>
<p>But wait, what if our company has proprietary data which cannot be shared with an outside vendor, and we want to use MCP to make it available as an MCP Tool to our Ai application?</p>
</blockquote>
<p>Our solution needs to be 100% local, in that it does not send a single bit of proprietary data to an external vendor, while still allowing us to create a nice ChaptGPT-like user interface with which to interact with our data.</p>
<p>So, we need to build our own MCP Client that can connect to any MCP Server on the Internet.</p>
<hr>
<h2 id="architecture-pinterest">Architecture (Pinterest)</h2>
<p>Inkscape</p>
<hr>
<h2 id="tech-stack">Tech Stack</h2>
<h3 id="llamaindex">LlamaIndex</h3>
<p>Orchestrate</p>
<h3 id="ollama-or-lmstudio">Ollama or LMStudio</h3>
<p>Serve LLM</p>
<h3 id="deepseek-r1">DeepSeek R1</h3>
<p>LLM we&rsquo;ll use (or gemma 3n)</p>
<hr>
<h2 id="getting-started">Getting Started</h2>
<p>We&rsquo;ll use the python package manager <strong>uv</strong> to setup our project.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#95a5a6"># verify installation</span>
</span></span><span style="display:flex;"><span>uv --version
</span></span><span style="display:flex;"><span><span style="color:#95a5a6"># pyproject.toml</span>
</span></span><span style="display:flex;"><span>makedir local_mcp
</span></span><span style="display:flex;"><span><span style="color:#728e00">cd</span> local_mcp
</span></span><span style="display:flex;"><span>uv init .
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#95a5a6"># instead of pip install, we write uv add</span>
</span></span><span style="display:flex;"><span>uv add 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#95a5a6"># we run things with: uv run</span>
</span></span></code></pre></div><blockquote>
<p>Before the next step we need to make sure that we actually have llama3 locally. We can do so with: <strong>ollama pull llama3.2</strong></p>
</blockquote>
<h3 id="install-llamaindex">Install LlamaIndex</h3>
<p>Let&rsquo;s say that we have an LLM,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#95a5a6"># We want to setup a local LLM agent using llama3.2 via Ollama and then connect it to our mcp tool server vis SSE (server-Sent Events).</span>
</span></span><span style="display:flex;"><span><span style="color:#95a5a6"># We first need to install the following:</span>
</span></span><span style="display:flex;"><span>uv add llama-index llama-index-llms-ollama nest_asyncio
</span></span></code></pre></div><p>Using <em>ollama</em> we can run a local version of <strong>Olama</strong>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>ollama run llama3
</span></span></code></pre></div><p>SSE events will be streamed on localhost at <em>http://127.0.0.1:8000/sse</em>.</p>
<hr>
<h2 id="run-the-application">Run the application</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>uv run server --server_type<span style="color:#728e00">=</span>sse
</span></span></code></pre></div><br>
<blockquote>
<p><strong>Preface:</strong> I recently coded a tiny Rust library,</p>
</blockquote>
<blockquote>
<p><strong>Code:</strong> The complete source code is available on my <a href="https://github.com/cryptopatrick/timed_release_crypto">GitHub</a>.</p>
</blockquote>
<!-- ////////////////////////////////////////////////////////////////////////-->
<h2 id="to-send-a-message-into-the-future">To send a message into the future</h2>
<p><figure><img src="../../../../../img/time-lock-message/rebellion.jpg">
</figure>
<br>
<strong>â–²<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></strong> A Rebel ship orbiting above the Federation&rsquo;s top secret military research lab, <em>The Periphery</em>.</p>
<p>â €â €â €â €â € This is a paragraph.
â €â €â €â €â € This is another paragraph.</p>
<p><strong>The Future</strong></p>
<p>If we&rsquo;re trying to send a message into the future, then we need to spend some</p>
<!-- ////////////////////////////////////////////////////////////////////////-->
<h3 id="title">Title</h3>
<p><strong>List of topics to cover (briefly):</strong> <br>
[ ] Carmicheal Numbers and the need for primality testing of huge numbers<br>
[x] Probabilitic Primality Testing with Rabin-Miller</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Step</th>
          <th style="text-align: left">Process</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><a href="#step-0-preparing-the-payload">0.</a></td>
          <td style="text-align: left">Gather message data, calculate time-lock duration, estimate attacker&rsquo;s squaring power</td>
      </tr>
      <tr>
          <td style="text-align: center"><a href="#step-1-compute-the-composite-modulus">1.</a></td>
          <td style="text-align: left">Choose two secret primes, $p$ and $q$, and compute the composite modulus: $n=p \cdot q$</td>
      </tr>
  </tbody>
</table>
<p><strong>â–²</strong> The seven steps of creating a Time-Lock Puzzle.</p>
<hr>
<h1 id="-implementation">ðŸ¦€ Implementation</h1>
<!-- ////////////////////////////////////////////////////////////////////////-->
<h4 id="step-0-preparing-the-payload">Step 0: Preparing the Payload</h4>
<p>$M$: <strong>Message to Protect</strong><br>
Dr. Angela Rae, and her team of fellow researchers, have quickly collected all</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ai-generated image of Rebel spaceship.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>




    </div>
    
    
  </div>
</section>

<section class="section">
  <div class="container has-text-centered">
    <p>'I write to understand as much as to be understood.' â€”Elie Wiesel<br> (c) 2024 CryptoPatrick</p>
  </div>
</section>

<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




</body>
